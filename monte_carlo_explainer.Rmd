---
title: "Monte Carlo"
output: html_document
---

After battling through a Monte Carlo example in a poorly designed Excel workbook at uni, I decided to put it all together in R to make sure I know how it really works. This won't cover too much of the **why** for Monte Carlo (the lecture notes cover that pretty well), rather it will cover the mechanics of how you make it work. This is demonstrably possible to do in Excel, but it's much easier to understand programmatically.

## The basics

The core idea of Monte Carlo simulation is that you are building a model which represents your beliefs about the world, and then using simulated data generated in accordance with those beliefs to understand the probable outcomes of that model. We'll work through an example of how all of this works, but for now the key points are:

* This technique does not (can not) use data. It is entirely based on your assumptions and beliefs about the world.
* This technique lets you encode your beliefs as probability distributions (rather than point estimates) which lets you reason about the uncertainty in your beliefs
* This technique produces a probability distribution as an output, which lets you understand the range of likely outcomes
* This technique lets you use numerical approximation methods to quickly estimate the solution to problems that would otherwise require complex analytical solutions, or may have no exact solution
* This technique makes no attempt to assess the relative sensitivity of the model to each of your assumptions, although performing such analysis would be a trivial extension of the technique.

## A worked example

Let's say we want to estimate the duration and cost of a project. We know some things about the tasks in this project:

1. There are 4 tasks. Tasks 1 and 2 can be completed at the same time, but task 3 depends on the outputs of tasks 1 and 2, and task 4 relies on the output of task 3.
1. The cost per day for each task is known.
1. The minimum, maximum, and most likely time taken to complete each task is able to be estimated based on business knowledge.

Let's start with the third point - the estimation of each task. Based on these three estimated values, we can simplify the problem by making an assumption that the uncertainty in our estimate of the time taken to complete each task follows a triangular distribution. If you're not familiar with probability distributions, this might require further explanation.

### Triangular distributions

A probability distribution represents uncertainty in a value. A triangular distribution looks like this:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(triangle)
n <- 10000
a <- 1
b <- 5
theta <- 2
X <- rtriangle(n, a, b, theta)
hist(X, breaks=50, col = "red", probability=TRUE)
lines(list(x = c(1,2,5), y = c(0,0.5,0)), col="blue", lwd=4)
```

In this plot the blue line represents my beliefs about the the values that X might be. I believe that it is between 1 and 5, and that it is most likely to be 2. The red bars are the distribution of observed values when I sampled 10,000 values from this distribution. This is the key bit of Monte Carlo - I can encode my beliefs as a distribution, and then sample from that distribution to model the uncertainty.

Let's take a deeper look at how the triangular distribution works.

The probability density function for a triangular distribution is fairly easy to determine using the rules we all learned about triangle geometry in primary school. Formally though, the distribution is defined as:

$$
\begin{cases}
    0 & \text{for } x < a, \\
    \frac{2(x-a)}{(b-a)(c-a)} & \text{for } a \le x < c, \\[4pt]
    \frac{2}{b-a}             & \text{for } x = c, \\[4pt]
    \frac{2(b-x)}{(b-a)(b-c)} & \text{for } c < x \le b, \\[4pt]
    0 & \text{for } b < x.
 \end{cases}
$$

where $a$ is the lowest value $x$ may take, $b$ is the highest value $x$ may take, and $c$ is your estimation of the most likely value for $x$. We can define this probability distribution as a function in R:

```{r message=FALSE, warning=FALSE}
library(dplyr)
triangle_pdf <- function(x, a, b, c) {
  case_when(
    (x < a) ~ 0,
    (x >= a & x < c) ~ (2*(x-a))/((b-a)*(c-a)),
    (x == c) ~ 2/(b-a),
    (x > c & x <= b) ~ (2*(b-x))/((b-a)*(b-c)),
    (x > b) ~ 0
  )
}
```

Let's plot some values for our function to make sure it's correct:

```{r}
plot(x = seq(0,6,0.1), y = triangle_pdf(x = seq(0,6,0.1), a = 1, b = 5, c = 2), type='l')
```

Good.

Now we need to work out how to sample from this distribution so that we can use these samples in a Monte Carlo. This isn't exactly straight forward - R gives us some ways to generate random numbers but none of these random number generators can draw from our new triangular distribution. In order to do this effectively, we need to first calculate the "cumulative density function" (CDF) for the distribution above. This function is the integral (area under the curve) for the figure above, and it estimates the probability that a point in distribution will be less than or equal to the x. 

Before we plot, it's probably worth thinking about this intuitively. The first section of the triangle is a positive-gradient straight line, so the integral of this section will be parabolic with an increasing gradient. The second section of the triangle is a negative-gradient straight line, so the integral of that section will also be parabolic but with a decreasing gradient. Together, these should look like a smooth S curve.

We can calculate this integral numerically to check that the intuition is correct.

```{r}
X <- seq(from = 0, to = 6, by = 0.01)
prob <- triangle_pdf(X, a = 1, b = 5, c = 2) * 0.01
plot(x = X, y = cumsum(prob), type='l')
```

Hooray! It matches muy intuition and looks just like the one on Wikipedia! We can actually calculate this directly using the formula for a cumulative density function, which is of course derived by integrating the PDF.

$$
\begin{cases}
    0 & \text{for } x \leq a, \\[2pt]
    \frac{(x-a)^2}{(b-a)(c-a)} & \text{for } a < x \leq c, \\[4pt]
    1-\frac{(b-x)^2}{(b-a)(b-c)} & \text{for } c < x < b, \\[4pt]
    1 & \text{for } b \leq x.
  \end{cases}
$$

And just like with the probability density function, we can implement this in R:

```{r}
triangle_cdf <- function(x, a, b, c) {
  case_when(
    (x <= a) ~ 0,
    (x > a & x <= c) ~ ((x-a)^2)/((b-a)*(c-a)),
    (x > c & x < b) ~ 1 - ((b-x)^2)/((b-a)*(b-c)),
    (x >= b) ~ 1
  )
}
```

Just like with the PDF, let's plot some values for our function to make sure it's correct and looks like the discrete approximation generated above:

```{r}
X = seq(0,6,0.1)
plot(x = X, y = triangle_cdf(X, a = 1, b = 5, c = 2), type='l')
```

Killing it!

Now we can look at why we're bothering with the CDF. R, much like Excel, has a couple of probability density functions built in, which you can sample from. The typical ones you might use are the normal distribution:

```{r}
hist(rnorm(100000), breaks=50, col = "red", probability=TRUE)
```

or the uniform distribution:

```{r}
hist(runif(100000), breaks=50, col = "red", probability=TRUE)
```

In this case, we can use our triangle CDF to transform points the uniform distribution into a triangle distribution! Unfortunately, this requires using the CDF "backwards", i.e. taking the inverse. Logically this looks like the following workflow:

1. Take a random point from the uniform distribution between 0 and 1
1. Find the value of X which gives this random point when you call **triangle_cdf**
1. Record that value of X and repeat

The values of X can be shown to be *drawn* from the triangle distribution. Let's do this process numerically to demonstrate how it works, before we cheat and look at how to do this analytically.

```{r}
inverse_cdf <- function(Y, a, b, c) {
  X_vals <- seq(from = 0, to = 6, by = 0.001)
  CDF_vals <- triangle_cdf(X_vals, a=a, b=b, c=c)
  idx <- rep(NA, length = length(Y))
  for (i in seq_along(Y)) {
    idx[i] <- which.min(abs(CDF_vals - Y[i]))
  }
  return(X_vals[idx])
}
```

```{r}
uniform_samples <- runif(10000, min = 0, max = 1)
triangle_samples <- inverse_cdf(uniform_samples, a = 1, b = 5, c = 2)
hist(triangle_samples, breaks=50, col = "red", probability=TRUE)
```

This is great! By creating our own "inverse CDF" function, we're now able to generate random data from a triange distribution, which is a key requirement for this Monte Carlo simulation. Now that we've shown how it works, we can cheat and use the analytically derived formula:

$$
X = \begin{cases}
a + \sqrt{U(b-a)(c-a)} & \text{ for } 0 < U < F(c) \\ & \\
b - \sqrt{(1-U)(b-a)(b-c)} & \text{ for } F(c) \le U < 1
\end{cases}
$$

where $F(c) = (c-a)/(b-a)$, has a triangular distribution with parameters $a, b$ and $c$.

```{r}
triangle_sample <- function(n, a, b, c) {
  
  # Generate some data from the uniform distribution
  U <- runif(n, min = 0, max = 1)
  
  # Use the inverse CDF to find the corresponding point from the triangular distribution
  Fc <- (c-a)/(b-a)
  case_when(
    (U > 0 & U < Fc) ~ a + sqrt(U * (b-a) * (c-a)),
    (U >= Fc & U < 1) ~ b - sqrt((1-U) * (b-a) * (b-c))
  )
}
```

```{r}
hist(triangle_sample(100000, a = 1, b = 5, c = 2), breaks=50, col = "red", probability=TRUE)
```

And if you're unfortunate enough to have to do this in Excel, you can make this a VBA function (which you can use in a spreadsheet) with the following code:

```{VBA}
Public Function triangle_rand(min As Double, max As Double, mostlikely As Double)

Dim U As Double
Dim Fc As Double
U = Rnd
Fc = (mostlikely - min) / (max - min)

If U > 0 And U < Fc Then
   triangle_rand = min + Sqr(U * (max - min) * (mostlikely - min))
ElseIf U >= Fc And U < 1 Then
  triangle_rand = max - Sqr((1 - U) * (max - min) * (max - mostlikely))
End If

End Function
```

Finally! Now we can get on with the Monte Carlo simulation.

### The rest of Monte Carlo

With a way to generate data from a triangle distribution, it's pretty straight-forward to build a simulation and estimate some values. Let's set out the problem again here:

1. There are 4 tasks. Tasks 1 and 2 can be completed at the same time, but task 3 depends on the outputs of tasks 1 and 2, and task 4 relies on the output of task 3.
1. The cost per day for each task is known.
1. The minimum, maximum, and most likely time taken to complete each task is able to be estimated based on business knowledge.

And here are the assumptions that we've been provided with:

| Task | Minimum Time | Maximum Time | Expected Time | Cost Per Day |
| ---- | ------------ | ------------ | ------------- | ------------ |
| 1    | 10           | 40           | 20            | $75          |
| 2    | 5            | 30           | 10            | $50          |
| 3    | 14           | 60           | 28            | $250         |
| 4    | 40           | 150          | 75            | $150         |

We also know that there is an overhead cost of $450 per day for project management.

We can now use this information to start calculating the duration and cost of the project by simulating each of these tasks independently.

```{r}
SIMULATIONS <- 1000000

task1_duration <- triangle_sample(n = SIMULATIONS, a = 10, b = 40,  c = 20)
task2_duration <- triangle_sample(n = SIMULATIONS, a = 5,  b = 30,  c = 10)
task3_duration <- triangle_sample(n = SIMULATIONS, a = 14, b = 60,  c = 28)
task4_duration <- triangle_sample(n = SIMULATIONS, a = 40, b = 150, c = 75)

par(mfrow = c(2,2))
hist(task1_duration, breaks=50, col = "red", probability=TRUE)
hist(task2_duration, breaks=50, col = "red", probability=TRUE)
hist(task3_duration, breaks=50, col = "red", probability=TRUE)
hist(task4_duration, breaks=50, col = "red", probability=TRUE)
```

We also need to calculate the overall project duration, which is governed by the following:

> Tasks 1 and 2 can be completed at the same time, but task 3 depends on the outputs of tasks 1 and 2, and task 4 relies on the output of task 3.

```{r}
task_1_2_duration <- pmax(task1_duration, task2_duration)
hist(task_1_2_duration, breaks=50, col = "red", probability=TRUE)
```

This is a good point to pause and consider the value of numerical simulations like Monte Carlo. Up until this moment we've been able to describe everything analytically, and the sampling wasn't really bringing anything new to the table. But when we consider the maximum time for task 1 and task 2, you can see that we've departed from a triange distribution and we're looking at some sort of hybrid distribution (I'm using 1,000,000 simulations to try and smooth it out as much as possible). I suspect that this distribution would be fairly difficult to describe analytically, but by using a numerical simulation we can generate and work with this hybrid distribution no matter the shape. This becomes even more interesting when we start combining more distributions together to get the duration of the whole project:

```{r}
project_duration <- task_1_2_duration + task3_duration + task4_duration
hist(project_duration, breaks=50, col = "red", probability=TRUE)
```

Now it looks like we're working with something that looks a bit like a skewed Gaussian distribution, and I was able to determine that without a single line of maths - brute force calculation did it for me. I don't need a mathematical description of the distribution, because I've got 1,000,000 simulated points from the distribution that I can use instead. If we aren't interested in the whole distribution, we can of course calculate some summary statistics to use for estimation:

```{r}
summary(project_duration)
```

With the task durations and project durations, we can now calculate the overall project cost:

```{r}
project_cost <- task1_duration * 75 +
                task2_duration * 50 +
                task3_duration * 250 +
                task4_duration * 150 +
                project_duration * 450
hist(project_cost, breaks=50, col = "red", probability=TRUE)
summary(project_cost)
```

By using a simulation, we can estimate the distribution of the project duration and project cost to a high degree of confidence, without having to determine the analytical solutions when we were performing arithmetic with the estimated probability density functions.

## Caveats

Whilst this all looks really cool mathematically, the use of simulations like Monte Carlo can give false confidence in the outcome. At their best, Monte Carlo simulations are a way to perform **arithmetic calculations that preserve uncertainty**. Some key things to keep in mind:

* Monte Carlo simulations are data-free; this is not a "data driven" technique by any stretch of the imagination
* There is no mechanism to update your beliefs using observed data
* The model is entirely based on your assumptions about the range of possible value and their relative probabilities. If your assumptions are invalid, then your results are invalid
* The use of triangle distributions is problematic, because they do not make any allowance for "black swan" events. The use of standard distributions (Gaussian, Poisson, etc) helps to resolve this problem due to their infinite tails.
* Triangle distributions are used because they are easier to specify using business knowledge - that doesn't mean they are good!
* Triangle distributions have the (perhaps unexpected) property that they will never generate outcomes at the minumum or maximum values. This means that whilst the "best case" and "worst case" estimates are probably reasonably likely in business contexts (like the project above), they will never be observed using a Monte Carlo method using triangle distributions because the probability of these values is set to zero by definition. The same goes for projects coming in under budget or over budget - by definition there is no chance for these outcomes to be generated.

